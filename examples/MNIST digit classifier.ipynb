{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digit classifier example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVIL HACK: Disable cuDNN check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: nvcc STDOUT mod.cu\r\n",
      "   Creating library D:/temp/theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmptrun7i/265abc51f7c376c224983485238ff1a5.lib and object D:/temp/theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmptrun7i/265abc51f7c376c224983485238ff1a5.exp\r\n",
      "\n",
      "Using gpu device 0: GeForce GTX 970 (CNMeM is enabled with initial size: 25.0% of memory, cuDNN 5103)\n",
      "d:\\packages\\theano\\theano\\sandbox\\cuda\\__init__.py:603: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn.cross_validation\n",
    "\n",
    "import lasagne\n",
    "import theano.tensor as T\n",
    "from britefury_lasagne import basic_dnn, trainer, image_window_extractor\n",
    "\n",
    "from fuel.datasets.mnist import MNIST\n",
    "import fuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network architecture\n",
    "\n",
    "We define the `build_network` function that takes the input variables as an optional argument and build the network using the Lasagne API.\n",
    "\n",
    "NOTE that the final dense layer does *NOT* use the `softmax` nonlinearity as it is supplied by the classifier builder (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(input_vars=None):\n",
    "    # Input layer\n",
    "    x_var = input_vars[0] if input_vars is not None else None\n",
    "    net = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=x_var)\n",
    "\n",
    "    # Two 32 unit 3x3 conv layers, followed by 2x2 max-pool\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(5, 5), W=lasagne.init.HeUniform())\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, pool_size=(2, 2))\n",
    "\n",
    "    # Two 32 unit 3x3 conv layers, followed by 2x2 max-pool\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3, 3), W=lasagne.init.HeUniform())\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3, 3), W=lasagne.init.HeUniform())\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 64 units followed by 50% dropout\n",
    "    net = lasagne.layers.DenseLayer(net, num_units=64, W=lasagne.init.HeUniform())\n",
    "    net = lasagne.layers.DropoutLayer(net, p=0.5)\n",
    "\n",
    "    # Final 10-unit dense layer, with no nonlinearity\n",
    "    net = lasagne.layers.DenseLayer(net, num_units=10, nonlinearity=None)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(which_sets=['train'], load_in_memory=True, subset=slice(0, 50000))\n",
    "mnist_val = MNIST(which_sets=['train'], load_in_memory=True, subset=slice(50000, None))\n",
    "mnist_test = MNIST(which_sets=['test'], load_in_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network\n",
      "Training\n",
      "Epoch 1/10 took 5.27s:  TRAIN y loss=0.424349  VAL y loss=0.078494 err=2.31%  TEST y loss=0.069636 err=2.09%\n",
      "Epoch 2/10 took 5.92s:  TRAIN y loss=0.141408  VAL y loss=0.057037 err=1.56%  TEST y loss=0.052428 err=1.66%\n",
      "Epoch 3/10 took 5.68s:  TRAIN y loss=0.101619  VAL y loss=0.044065 err=1.32%  TEST y loss=0.035886 err=1.16%\n",
      "Epoch 4/10 took 5.71s:  TRAIN y loss=0.079419  VAL y loss=0.047646 err=1.30%  TEST y loss=0.032739 err=1.04%\n",
      "Epoch 5/10 took 5.48s:  TRAIN y loss=0.071349  VAL y loss=0.044890 err=1.34%\n",
      "Epoch 6/10 took 5.32s:  TRAIN y loss=0.060336  VAL y loss=0.040141 err=1.08%  TEST y loss=0.027523 err=0.92%\n",
      "Epoch 7/10 took 6.08s:  TRAIN y loss=0.053391  VAL y loss=0.036406 err=1.01%  TEST y loss=0.022415 err=0.73%\n",
      "Epoch 8/10 took 5.51s:  TRAIN y loss=0.047946  VAL y loss=0.033787 err=0.84%  TEST y loss=0.025142 err=0.85%\n",
      "Epoch 9/10 took 5.58s:  TRAIN y loss=0.045110  VAL y loss=0.032317 err=0.88%\n",
      "Epoch 10/10 took 5.14s:  TRAIN y loss=0.039169  VAL y loss=0.036157 err=1.05%\n",
      "Final result:\n",
      "Epoch 8/10 took 55.69s:  TRAIN y loss=0.047946  VAL y loss=0.033787 err=0.84%  TEST y loss=0.025142 err=0.85%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<britefury_lasagne.trainer.TrainingResults at 0x7d6056d8>"
      ]
     },
     "execution_count": 5,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "# Build the image classifier for the given model builder\n",
    "print 'Building network'\n",
    "clf = basic_dnn.simple_classifier(build_network, n_input_spatial_dims=2, target_channel_index=0,\n",
    "            updates_fn=lambda loss, params: lasagne.updates.adam(loss, params, learning_rate=0.001))\n",
    "\n",
    "# Set verbosity\n",
    "clf.trainer.report(verbosity=trainer.VERBOSITY_EPOCH)\n",
    "\n",
    "# Set training length\n",
    "clf.trainer.train_for(num_epochs=10)\n",
    "\n",
    "# Train\n",
    "print 'Training'\n",
    "clf.trainer.train(mnist_train, mnist_val, mnist_test, batchsize=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the classifier to predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error rate=0.85%\n"
     ]
    }
   ],
   "source": [
    "# Get a Fuel dataset for MNIST test set, features only (no targets)\n",
    "mnist_test_features = MNIST(which_sets=['test'], sources=['features'], load_in_memory=True)\n",
    "\n",
    "# Predict probabilities for test samples\n",
    "test_y_pred_prob = clf.predict(mnist_test_features)[0]\n",
    "# Use `np.argmax` to get class predictions\n",
    "test_y_pred = np.argmax(test_y_pred_prob, axis=1)\n",
    "\n",
    "# Get the ground truths\n",
    "state = mnist_test.open()\n",
    "test_y = mnist_test.get_data(state, request=slice(None))[1]\n",
    "\n",
    "# Show the error rate\n",
    "print 'Test error rate={:.2%}'.format(np.mean(test_y_pred != test_y[:,0]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}