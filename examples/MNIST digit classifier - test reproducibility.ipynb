{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digit classifier example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn of cuDNN as its convolution operations are *not reproducible*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'dnn.enabled=False, optimizer_including='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is enabled with initial size: 25.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn.cross_validation\n",
    "\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from britefury_lasagne import basic_dnn, trainer, image_window_extractor, mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network architecture\n",
    "\n",
    "We define the `build_network` function that takes the input variables as an optional argument and build the network using the Lasagne API.\n",
    "\n",
    "NOTE that the final dense layer does *NOT* use the `softmax` nonlinearity as it is supplied by the classifier builder (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network(input_vars=None):\n",
    "    # Input layer\n",
    "    x_var = input_vars[0] if input_vars is not None else None\n",
    "    net = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=x_var)\n",
    "\n",
    "    # A 32 unit 5x5 conv layer, followed by 2x2 max-pool\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(5, 5), W=lasagne.init.HeUniform(), name='c1_1')\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, pool_size=(2, 2), name='p1')\n",
    "\n",
    "    # Two 32 unit 3x3 conv layers, followed by 2x2 max-pool\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3, 3), W=lasagne.init.HeUniform(), name='c2_1')\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3, 3), W=lasagne.init.HeUniform(), name='c2_2')\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, pool_size=(2, 2), name='p2')\n",
    "\n",
    "    # A fully-connected layer of 64 units\n",
    "    net = lasagne.layers.DenseLayer(net, num_units=64, W=lasagne.init.HeUniform(), name='d3')\n",
    "    \n",
    "    # NO DROPOUT; dropout state is difficult to reset/restore in such a way as to ensure reproducibility;\n",
    "    # Even `dropout_layer._srng.set_rstate(some_constant)` does not seem to work\n",
    "    # net = lasagne.layers.DropoutLayer(net, p=0.5)\n",
    "\n",
    "    # Final 10-unit dense layer, with no nonlinearity\n",
    "    net = lasagne.layers.DenseLayer(net, num_units=10, nonlinearity=None, name='d4')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = mnist.MNIST()\n",
    "\n",
    "# Select a small subset. We are testing for reproducibility of computations;\n",
    "# classifier accuracy is not really important.\n",
    "train_X = dataset.train_X[:1000]\n",
    "train_y = dataset.train_y[:1000]\n",
    "val_X = dataset.val_X[:1000]\n",
    "val_y = dataset.val_y[:1000]\n",
    "test_X = dataset.test_X[:1000]\n",
    "test_y = dataset.test_y[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network\n",
      "Training\n",
      "Epoch 1/5 took 0.24s:  TRAIN y loss=2.085214  VAL y loss=1.740353 err=42.10%  TEST y loss=1.758568 err=44.90%\n",
      "Epoch 2/5 took 0.24s:  TRAIN y loss=1.323869  VAL y loss=0.978611 err=30.20%  TEST y loss=0.976066 err=29.20%\n",
      "Epoch 3/5 took 0.25s:  TRAIN y loss=0.670521  VAL y loss=0.724290 err=23.60%  TEST y loss=0.673940 err=21.30%\n",
      "Epoch 4/5 took 0.24s:  TRAIN y loss=0.455425  VAL y loss=0.610251 err=20.60%  TEST y loss=0.562029 err=18.90%\n",
      "Epoch 5/5 took 0.23s:  TRAIN y loss=0.347692  VAL y loss=0.464677 err=15.50%  TEST y loss=0.438299 err=14.30%\n",
      "Final result:\n",
      "Epoch 5/5 took 1.20s:  TRAIN y loss=0.347692  VAL y loss=0.464677 err=15.50%  TEST y loss=0.438299 err=14.30%\n"
     ]
    }
   ],
   "source": [
    "# Create SEPARATE random number generators for generating weights and shuffling,\n",
    "# with specific seeds so that we can re-try the experiment below to check that we get the same result\n",
    "# Creating a network's layers draws from an RNG in order to create randomly initialised network weights.\n",
    "# Since the third experiment does not create new layers, we need separate RNGs for the shuffling\n",
    "# step so that we can ensure that it operates the same way each time\n",
    "weight_rng1 = np.random.RandomState(12345)\n",
    "shuffle_rng1 = np.random.RandomState(24680)\n",
    "\n",
    "# Set Lasagne's RNG\n",
    "lasagne.random.set_rng(weight_rng1)\n",
    "\n",
    "# Build the image classifier for the given model builder\n",
    "print('Building network')\n",
    "clf = basic_dnn.simple_classifier(build_network, n_input_spatial_dims=2,\n",
    "            updates_fn=lambda loss, params: lasagne.updates.adam(loss, params, learning_rate=0.001))\n",
    "\n",
    "# Get the randomly initialised parameter values so that we can try to re-start training without building\n",
    "# the net from scratch\n",
    "blank_state = clf.get_param_values(include_updates=True)\n",
    "\n",
    "# Set verbosity\n",
    "clf.trainer.report(verbosity=trainer.VERBOSITY_EPOCH)\n",
    "\n",
    "# Set training length\n",
    "clf.trainer.train_for(num_epochs=5)\n",
    "\n",
    "# Train\n",
    "print('Training')\n",
    "clf.trainer.train([train_X, train_y], [val_X, val_y], [test_X, test_y], batchsize=128, shuffle_rng=shuffle_rng1)\n",
    "\n",
    "clf_state = clf.get_param_values(include_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network\n",
      "Training\n",
      "Epoch 1/5 took 0.27s:  TRAIN y loss=2.085214  VAL y loss=1.740353 err=42.10%  TEST y loss=1.758568 err=44.90%\n",
      "Epoch 2/5 took 0.38s:  TRAIN y loss=1.323869  VAL y loss=0.978611 err=30.20%  TEST y loss=0.976066 err=29.20%\n",
      "Epoch 3/5 took 0.36s:  TRAIN y loss=0.670521  VAL y loss=0.724290 err=23.60%  TEST y loss=0.673940 err=21.30%\n",
      "Epoch 4/5 took 0.37s:  TRAIN y loss=0.455425  VAL y loss=0.610251 err=20.60%  TEST y loss=0.562029 err=18.90%\n",
      "Epoch 5/5 took 0.36s:  TRAIN y loss=0.347692  VAL y loss=0.464677 err=15.50%  TEST y loss=0.438299 err=14.30%\n",
      "Final result:\n",
      "Epoch 5/5 took 1.74s:  TRAIN y loss=0.347692  VAL y loss=0.464677 err=15.50%  TEST y loss=0.438299 err=14.30%\n"
     ]
    }
   ],
   "source": [
    "weight_rng2 = np.random.RandomState(12345)\n",
    "shuffle_rng2 = np.random.RandomState(24680)\n",
    "\n",
    "lasagne.random.set_rng(weight_rng2)\n",
    "\n",
    "# Build the image classifier for the given model builder\n",
    "print('Building network')\n",
    "clf2 = basic_dnn.simple_classifier(build_network, n_input_spatial_dims=2,\n",
    "            updates_fn=lambda loss, params: lasagne.updates.adam(loss, params, learning_rate=0.001))\n",
    "\n",
    "# Set verbosity\n",
    "clf2.trainer.report(verbosity=trainer.VERBOSITY_EPOCH)\n",
    "\n",
    "# Set training length\n",
    "clf2.trainer.train_for(num_epochs=5)\n",
    "\n",
    "# Train\n",
    "print('Training')\n",
    "clf2.trainer.train([train_X, train_y], [val_X, val_y], [test_X, test_y], batchsize=128, shuffle_rng=shuffle_rng2)\n",
    "\n",
    "clf2_state = clf2.get_param_values(include_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the state of the first classifier and train again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1/5 took 0.36s:  TRAIN y loss=2.085214  VAL y loss=1.740353 err=42.10%  TEST y loss=1.758568 err=44.90%\n",
      "Epoch 2/5 took 0.37s:  TRAIN y loss=1.323869  VAL y loss=0.978611 err=30.20%  TEST y loss=0.976066 err=29.20%\n",
      "Epoch 3/5 took 0.40s:  TRAIN y loss=0.670521  VAL y loss=0.724290 err=23.60%  TEST y loss=0.673940 err=21.30%\n",
      "Epoch 4/5 took 0.39s:  TRAIN y loss=0.455425  VAL y loss=0.610251 err=20.60%  TEST y loss=0.562029 err=18.90%\n",
      "Epoch 5/5 took 0.37s:  TRAIN y loss=0.347692  VAL y loss=0.464677 err=15.50%  TEST y loss=0.438299 err=14.30%\n",
      "Final result:\n",
      "Epoch 5/5 took 1.90s:  TRAIN y loss=0.347692  VAL y loss=0.464677 err=15.50%  TEST y loss=0.438299 err=14.30%\n"
     ]
    }
   ],
   "source": [
    "shuffle_rng3 = np.random.RandomState(24680)\n",
    "\n",
    "# Reset parameter state\n",
    "clf.set_param_values(blank_state, include_updates=True)\n",
    "\n",
    "# Train\n",
    "print('Training')\n",
    "clf.trainer.train([train_X, train_y], [val_X, val_y], [test_X, test_y], batchsize=128, shuffle_rng=shuffle_rng3)\n",
    "\n",
    "clf_state_b = clf.get_param_values(include_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters for equality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_states(s1, s2):\n",
    "    success = True\n",
    "    for i, (a, b) in enumerate(zip(s1, s2)):\n",
    "        if (a != b).any():\n",
    "            print('FAIL at index {}/{}'.format(i, len(s1)))\n",
    "            success = False\n",
    "    if success:\n",
    "        print('States are identical')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States are identical\n"
     ]
    }
   ],
   "source": [
    "compare_states(clf_state, clf2_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States are identical\n"
     ]
    }
   ],
   "source": [
    "compare_states(clf_state, clf_state_b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
