{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digit classifier example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn of cuDNN as its convolution operations are *not reproducible*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'dnn.enabled=False, optimizer_including='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970 (CNMeM is enabled with initial size: 25.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn.cross_validation\n",
    "\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from britefury_lasagne import basic_dnn, trainer, image_window_extractor\n",
    "\n",
    "from fuel.datasets.mnist import MNIST\n",
    "import fuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network architecture\n",
    "\n",
    "We define the `build_network` function that takes the input variables as an optional argument and build the network using the Lasagne API.\n",
    "\n",
    "NOTE that the final dense layer does *NOT* use the `softmax` nonlinearity as it is supplied by the classifier builder (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network(input_vars=None):\n",
    "    # Input layer\n",
    "    x_var = input_vars[0] if input_vars is not None else None\n",
    "    net = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=x_var)\n",
    "\n",
    "    # A 32 unit 5x5 conv layer, followed by 2x2 max-pool\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(5, 5), W=lasagne.init.HeUniform(), name='c1_1')\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, pool_size=(2, 2), name='p1')\n",
    "\n",
    "    # Two 32 unit 3x3 conv layers, followed by 2x2 max-pool\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3, 3), W=lasagne.init.HeUniform(), name='c2_1')\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3, 3), W=lasagne.init.HeUniform(), name='c2_2')\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, pool_size=(2, 2), name='p2')\n",
    "\n",
    "    # A fully-connected layer of 64 units\n",
    "    net = lasagne.layers.DenseLayer(net, num_units=64, W=lasagne.init.HeUniform(), name='d3')\n",
    "    \n",
    "    # NO DROPOUT; dropout state is difficult to reset/restore in such a way as to ensure reproducibility;\n",
    "    # Even `dropout_layer._srng.set_rstate(some_constant)` does not seem to work\n",
    "    # net = lasagne.layers.DropoutLayer(net, p=0.5)\n",
    "\n",
    "    # Final 10-unit dense layer, with no nonlinearity\n",
    "    net = lasagne.layers.DenseLayer(net, num_units=10, nonlinearity=None, name='d4')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_train = MNIST(which_sets=['train'], load_in_memory=True, subset=slice(0, 1000))\n",
    "mnist_val = MNIST(which_sets=['train'], load_in_memory=True, subset=slice(1000, 2000))\n",
    "mnist_test = MNIST(which_sets=['test'], load_in_memory=True, subset=slice(0, 1000))\n",
    "\n",
    "train_X, train_y = mnist_train.get_data(mnist_train.open(), request=slice(0, 1000))\n",
    "val_X, val_y = mnist_val.get_data(mnist_train.open(), request=slice(0, 1000))\n",
    "test_X, test_y = mnist_test.get_data(mnist_train.open(), request=slice(0, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network\n",
      "Training\n",
      "Epoch 1/5 took 0.30s:  TRAIN y loss=39.954645  VAL y loss=13.138603 err=67.20%  TEST y loss=15.358888 err=72.50%\n",
      "Epoch 2/5 took 0.28s:  TRAIN y loss=7.458300  VAL y loss=5.213586 err=47.00%  TEST y loss=5.733494 err=52.00%\n",
      "Epoch 3/5 took 0.28s:  TRAIN y loss=3.116188  VAL y loss=2.710061 err=37.40%  TEST y loss=3.158632 err=40.50%\n",
      "Epoch 4/5 took 0.28s:  TRAIN y loss=1.593666  VAL y loss=1.950297 err=28.00%  TEST y loss=2.298699 err=32.30%\n",
      "Epoch 5/5 took 0.28s:  TRAIN y loss=0.936020  VAL y loss=1.507651 err=23.80%  TEST y loss=1.797327 err=26.20%\n",
      "Final result:\n",
      "Epoch 5/5 took 1.43s:  TRAIN y loss=0.936020  VAL y loss=1.507651 err=23.80%  TEST y loss=1.797327 err=26.20%\n"
     ]
    }
   ],
   "source": [
    "# Create SEPARATE random number generators for generating weights and shuffling,\n",
    "# with specific seeds so that we can re-try the experiment below to check that we get the same result\n",
    "# Creating a network's layers draws from an RNG in order to create randomly initialised network weights.\n",
    "# Since the third experiment does not create new layers, we need separate RNGs for the shuffling\n",
    "# step so that we can ensure that it operates the same way each time\n",
    "weight_rng1 = np.random.RandomState(12345)\n",
    "shuffle_rng1 = np.random.RandomState(24680)\n",
    "\n",
    "# Set Lasagne's RNG\n",
    "lasagne.random.set_rng(weight_rng1)\n",
    "\n",
    "# Build the image classifier for the given model builder\n",
    "print 'Building network'\n",
    "clf = basic_dnn.simple_classifier(build_network, n_input_spatial_dims=2, target_channel_index=0,\n",
    "            updates_fn=lambda loss, params: lasagne.updates.adam(loss, params, learning_rate=0.001))\n",
    "\n",
    "# Get the randomly initialised parameter values so that we can try to re-start training without building\n",
    "# the net from scratch\n",
    "blank_state = clf.get_param_values(include_updates=True)\n",
    "\n",
    "# Set verbosity\n",
    "clf.trainer.report(verbosity=trainer.VERBOSITY_EPOCH)\n",
    "\n",
    "# Set training length\n",
    "clf.trainer.train_for(num_epochs=5)\n",
    "\n",
    "# Train\n",
    "print 'Training'\n",
    "clf.trainer.train([train_X, train_y], [val_X, val_y], [test_X, test_y], batchsize=128, shuffle_rng=shuffle_rng1)\n",
    "\n",
    "clf_state = clf.get_param_values(include_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network\n",
      "Training\n",
      "Epoch 1/5 took 0.40s:  TRAIN y loss=39.954645  VAL y loss=13.138603 err=67.20%  TEST y loss=15.358888 err=72.50%\n",
      "Epoch 2/5 took 0.30s:  TRAIN y loss=7.458300  VAL y loss=5.213586 err=47.00%  TEST y loss=5.733494 err=52.00%\n",
      "Epoch 3/5 took 0.28s:  TRAIN y loss=3.116188  VAL y loss=2.710061 err=37.40%  TEST y loss=3.158632 err=40.50%\n",
      "Epoch 4/5 took 0.28s:  TRAIN y loss=1.593666  VAL y loss=1.950297 err=28.00%  TEST y loss=2.298699 err=32.30%\n",
      "Epoch 5/5 took 0.28s:  TRAIN y loss=0.936020  VAL y loss=1.507651 err=23.80%  TEST y loss=1.797327 err=26.20%\n",
      "Final result:\n",
      "Epoch 5/5 took 1.54s:  TRAIN y loss=0.936020  VAL y loss=1.507651 err=23.80%  TEST y loss=1.797327 err=26.20%\n"
     ]
    }
   ],
   "source": [
    "weight_rng2 = np.random.RandomState(12345)\n",
    "shuffle_rng2 = np.random.RandomState(24680)\n",
    "\n",
    "lasagne.random.set_rng(weight_rng2)\n",
    "\n",
    "# Build the image classifier for the given model builder\n",
    "print 'Building network'\n",
    "clf2 = basic_dnn.simple_classifier(build_network, n_input_spatial_dims=2, target_channel_index=0,\n",
    "            updates_fn=lambda loss, params: lasagne.updates.adam(loss, params, learning_rate=0.001))\n",
    "\n",
    "# Set verbosity\n",
    "clf2.trainer.report(verbosity=trainer.VERBOSITY_EPOCH)\n",
    "\n",
    "# Set training length\n",
    "clf2.trainer.train_for(num_epochs=5)\n",
    "\n",
    "# Train\n",
    "print 'Training'\n",
    "clf2.trainer.train([train_X, train_y], [val_X, val_y], [test_X, test_y], batchsize=128, shuffle_rng=shuffle_rng2)\n",
    "\n",
    "clf2_state = clf2.get_param_values(include_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the state of the first classifier and train again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1/5 took 0.28s:  TRAIN y loss=39.954645  VAL y loss=13.138603 err=67.20%  TEST y loss=15.358888 err=72.50%\n",
      "Epoch 2/5 took 0.29s:  TRAIN y loss=7.458300  VAL y loss=5.213586 err=47.00%  TEST y loss=5.733494 err=52.00%\n",
      "Epoch 3/5 took 0.31s:  TRAIN y loss=3.116188  VAL y loss=2.710061 err=37.40%  TEST y loss=3.158632 err=40.50%\n",
      "Epoch 4/5 took 0.28s:  TRAIN y loss=1.593666  VAL y loss=1.950297 err=28.00%  TEST y loss=2.298699 err=32.30%\n",
      "Epoch 5/5 took 0.28s:  TRAIN y loss=0.936020  VAL y loss=1.507651 err=23.80%  TEST y loss=1.797327 err=26.20%\n",
      "Final result:\n",
      "Epoch 5/5 took 1.43s:  TRAIN y loss=0.936020  VAL y loss=1.507651 err=23.80%  TEST y loss=1.797327 err=26.20%\n"
     ]
    }
   ],
   "source": [
    "shuffle_rng3 = np.random.RandomState(24680)\n",
    "\n",
    "# Reset parameter state\n",
    "clf.set_param_values(blank_state, include_updates=True)\n",
    "\n",
    "# Train\n",
    "print 'Training'\n",
    "clf.trainer.train([train_X, train_y], [val_X, val_y], [test_X, test_y], batchsize=128, shuffle_rng=shuffle_rng3)\n",
    "\n",
    "clf_state_b = clf.get_param_values(include_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters for equality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_states(s1, s2):\n",
    "    for i, (a, b) in enumerate(zip(s1, s2)):\n",
    "        if (a != b).any():\n",
    "            print 'FAIL at index {}/{}'.format(i, len(s1))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_states(clf_state, clf2_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_states(clf_state, clf2_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_states(clf_state, clf_state_b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
