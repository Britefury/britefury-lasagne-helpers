{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digit classifier - test reproducibility\n",
    "\n",
    "In this notebook, we test reproducibility of training by:\n",
    "- Training a network on a small subset of the MNIST dataset and getting the values of its parameters\n",
    "- Training a second network using the same initial conditions and getting the values of its parameters, then ensuring taht the parameter values match between the two networks\n",
    "- Restoring the state of the parameters of the first network to their initial values then training it again. The resulting parameter values are acquired and compared against the parameter values of the first training run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn of cuDNN as its convolution operations are *not reproducible*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'dnn.enabled=False, optimizer_including='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Geoff/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn.cross_validation\n",
    "\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from britefury_lasagne import basic_dnn, trainer, image_window_extractor, mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network architecture\n",
    "\n",
    "We define the `build_network` function that takes the input variables as an optional argument and build the network using the Lasagne API.\n",
    "\n",
    "NOTE that the final dense layer does *NOT* use the `softmax` nonlinearity as it is supplied by the classifier builder (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network(input_vars=None):\n",
    "    # Input layer\n",
    "    x_var = input_vars[0] if input_vars is not None else None\n",
    "    net = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=x_var)\n",
    "\n",
    "    # A 32 unit 5x5 conv layer, followed by 2x2 max-pool\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(5, 5), W=lasagne.init.HeUniform(), name='c1_1')\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, pool_size=(2, 2), name='p1')\n",
    "\n",
    "    # Two 32 unit 3x3 conv layers, followed by 2x2 max-pool\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3, 3), W=lasagne.init.HeUniform(), name='c2_1')\n",
    "    net = lasagne.layers.Conv2DLayer(net, num_filters=32, filter_size=(3, 3), W=lasagne.init.HeUniform(), name='c2_2')\n",
    "    net = lasagne.layers.MaxPool2DLayer(net, pool_size=(2, 2), name='p2')\n",
    "\n",
    "    # A fully-connected layer of 64 units\n",
    "    net = lasagne.layers.DenseLayer(net, num_units=64, W=lasagne.init.HeUniform(), name='d3')\n",
    "    \n",
    "    # NO DROPOUT; dropout state is difficult to reset/restore in such a way as to ensure reproducibility;\n",
    "    # Even `dropout_layer._srng.set_rstate(some_constant)` does not seem to work\n",
    "    # net = lasagne.layers.DropoutLayer(net, p=0.5)\n",
    "\n",
    "    # Final 10-unit dense layer, with no nonlinearity\n",
    "    net = lasagne.layers.DenseLayer(net, num_units=10, nonlinearity=None, name='d4')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = mnist.MNIST()\n",
    "\n",
    "# Select a small subset. We are testing for reproducibility of computations;\n",
    "# classifier accuracy is not really important.\n",
    "train_X = dataset.train_X[:512]\n",
    "train_y = dataset.train_y[:512]\n",
    "val_X = dataset.val_X[:512]\n",
    "val_y = dataset.val_y[:512]\n",
    "test_X = dataset.test_X[:512]\n",
    "test_y = dataset.test_y[:512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network\n",
      "Training\n",
      "Epoch 1 took 0.94s:  TRAIN y loss=2.219832  VAL y loss=2.083671 err=53.71%  TEST y loss=2.079519 err=52.73%\n",
      "Epoch 2 took 0.95s:  TRAIN y loss=1.914864  VAL y loss=1.759531 err=45.90%  TEST y loss=1.747002 err=45.12%\n",
      "Epoch 3 took 1.01s:  TRAIN y loss=1.489491  VAL y loss=1.341996 err=34.96%  TEST y loss=1.317358 err=34.57%\n",
      "Epoch 4 took 1.02s:  TRAIN y loss=1.014338  VAL y loss=0.995141 err=28.71%  TEST y loss=0.964328 err=29.30%\n",
      "Epoch 5 took 0.89s:  TRAIN y loss=0.671612  VAL y loss=0.845743 err=26.95%  TEST y loss=0.780863 err=27.54%\n",
      "Best result:\n",
      "Epoch 5 took 4.82s:  TRAIN y loss=0.671612  VAL y loss=0.845743 err=26.95%  TEST y loss=0.780863 err=27.54%\n"
     ]
    }
   ],
   "source": [
    "# Create SEPARATE random number generators for generating weights and shuffling,\n",
    "# with specific seeds so that we can re-try the experiment below to check that we get the same result\n",
    "# Creating a network's layers draws from an RNG in order to create randomly initialised network weights.\n",
    "# Since the third experiment does not create new layers, we need separate RNGs for the shuffling\n",
    "# step so that we can ensure that it operates the same way each time\n",
    "weight_rng1 = np.random.RandomState(12345)\n",
    "shuffle_rng1 = np.random.RandomState(24680)\n",
    "\n",
    "# Set Lasagne's RNG\n",
    "lasagne.random.set_rng(weight_rng1)\n",
    "\n",
    "# Build the image classifier for the given model builder\n",
    "print('Building network')\n",
    "clf = basic_dnn.simple_classifier(build_network, n_input_spatial_dims=2,\n",
    "            updates_fn=lambda loss, params: lasagne.updates.adam(loss, params, learning_rate=0.001))\n",
    "\n",
    "# Get the randomly initialised parameter values so that we can try to re-start training without building\n",
    "# the net from scratch\n",
    "blank_state = clf.get_param_values(include_updates=True)\n",
    "\n",
    "# Train with a batch size of 128, for 5 epochs, reporting after each epoch,\n",
    "# using a provided random number generator for shuffling to ensure reproducibility\n",
    "print('Training')\n",
    "clf.train([train_X, train_y], [val_X, val_y], [test_X, test_y],\n",
    "          batchsize=128, num_epochs=5, verbosity=trainer.VERBOSITY_EPOCH,\n",
    "          shuffle_rng=shuffle_rng1)\n",
    "\n",
    "clf_state = clf.get_param_values(include_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network\n",
      "Training\n",
      "Epoch 1 took 1.00s:  TRAIN y loss=2.219832  VAL y loss=2.083671 err=53.71%  TEST y loss=2.079519 err=52.73%\n",
      "Epoch 2 took 0.99s:  TRAIN y loss=1.914864  VAL y loss=1.759531 err=45.90%  TEST y loss=1.747002 err=45.12%\n",
      "Epoch 3 took 0.92s:  TRAIN y loss=1.489491  VAL y loss=1.341996 err=34.96%  TEST y loss=1.317358 err=34.57%\n",
      "Epoch 4 took 0.81s:  TRAIN y loss=1.014338  VAL y loss=0.995141 err=28.71%  TEST y loss=0.964328 err=29.30%\n",
      "Epoch 5 took 0.79s:  TRAIN y loss=0.671612  VAL y loss=0.845743 err=26.95%  TEST y loss=0.780863 err=27.54%\n",
      "Best result:\n",
      "Epoch 5 took 4.50s:  TRAIN y loss=0.671612  VAL y loss=0.845743 err=26.95%  TEST y loss=0.780863 err=27.54%\n"
     ]
    }
   ],
   "source": [
    "weight_rng2 = np.random.RandomState(12345)\n",
    "shuffle_rng2 = np.random.RandomState(24680)\n",
    "\n",
    "lasagne.random.set_rng(weight_rng2)\n",
    "\n",
    "# Build the image classifier for the given model builder\n",
    "print('Building network')\n",
    "clf2 = basic_dnn.simple_classifier(build_network, n_input_spatial_dims=2,\n",
    "            updates_fn=lambda loss, params: lasagne.updates.adam(loss, params, learning_rate=0.001))\n",
    "\n",
    "# Train\n",
    "print('Training')\n",
    "clf2.train([train_X, train_y], [val_X, val_y], [test_X, test_y],\n",
    "           batchsize=128, num_epochs=5, verbosity=trainer.VERBOSITY_EPOCH,\n",
    "           shuffle_rng=shuffle_rng2)\n",
    "\n",
    "clf2_state = clf2.get_param_values(include_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the state of the first classifier and train again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1 took 0.81s:  TRAIN y loss=2.219832  VAL y loss=2.083671 err=53.71%  TEST y loss=2.079519 err=52.73%\n",
      "Epoch 2 took 0.89s:  TRAIN y loss=1.914864  VAL y loss=1.759531 err=45.90%  TEST y loss=1.747002 err=45.12%\n",
      "Epoch 3 took 0.98s:  TRAIN y loss=1.489491  VAL y loss=1.341996 err=34.96%  TEST y loss=1.317358 err=34.57%\n",
      "Epoch 4 took 0.91s:  TRAIN y loss=1.014338  VAL y loss=0.995141 err=28.71%  TEST y loss=0.964328 err=29.30%\n",
      "Epoch 5 took 0.79s:  TRAIN y loss=0.671612  VAL y loss=0.845743 err=26.95%  TEST y loss=0.780863 err=27.54%\n",
      "Best result:\n",
      "Epoch 5 took 4.39s:  TRAIN y loss=0.671612  VAL y loss=0.845743 err=26.95%  TEST y loss=0.780863 err=27.54%\n"
     ]
    }
   ],
   "source": [
    "shuffle_rng3 = np.random.RandomState(24680)\n",
    "\n",
    "# Reset parameter state\n",
    "clf.set_param_values(blank_state, include_updates=True)\n",
    "\n",
    "# Train\n",
    "print('Training')\n",
    "clf.train([train_X, train_y], [val_X, val_y], [test_X, test_y],\n",
    "          batchsize=128, num_epochs=5, verbosity=trainer.VERBOSITY_EPOCH,\n",
    "          shuffle_rng=shuffle_rng3)\n",
    "\n",
    "clf_state_b = clf.get_param_values(include_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters for equality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_states(s1, s2):\n",
    "    success = True\n",
    "    for i, (a, b) in enumerate(zip(s1, s2)):\n",
    "        if (a != b).any():\n",
    "            print('FAIL at index {}/{}'.format(i, len(s1)))\n",
    "            success = False\n",
    "    if success:\n",
    "        print('States are identical')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States are identical\n"
     ]
    }
   ],
   "source": [
    "compare_states(clf_state, clf2_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States are identical\n"
     ]
    }
   ],
   "source": [
    "compare_states(clf_state, clf_state_b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
